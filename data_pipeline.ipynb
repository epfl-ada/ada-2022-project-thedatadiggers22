{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapipeline\n",
    "### 1.Motivation\n",
    " #### Because we want to analyse the \"digital\" propagation of Covid-19 around the world, we need more countries than used in the original dataset and on a longer. Thus, we decided to rebuild a dataset considering page views on covid related articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from load_helper_CoronaWiki import *\n",
    "import urllib.parse\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Initial build of covid related article list\n",
    "\n",
    "Because the original list of article related to Covid was not available anymore (https://covid-data.wmflabs.org), we used the same method used to build this list to get a new list (method explained here: https://public.paws.wmcloud.org/User:Diego_(WMF)/CoronaAllRelatedPagesMarch30.ipynb). To summarize, the method consist of take all Wikidata Articles that links to a main COVID-19 pages COVID-19 (Q84263196) and the 2019–20 COVID-19 pandemic (Q81068910) and removing the 'human' instances of this pages.\n",
    "\n",
    "However we contacted in the mean time the responsible of the dashboard about Covid-19 related articles page views (https://analytics.wikimedia.org/published/dashboards/Wikipedia_C-19_Comms_Stats/), Shay Nowick who kindly sent us the original list(COVID_related_pages_project.csv). Thus, we used this list to design our data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what links to COVID-19 (Q84263196) in Wikidata\n",
    "\n",
    "whatLinks = []\n",
    "\n",
    "# COVID -19\n",
    "url = 'https://www.wikidata.org/w/api.php?action=query&format=json&list=backlinks&bltitle=Q84263196&bllimit=500&blnamespace=0'\n",
    "response = requests.get(url=url).json()\n",
    "whatLinks.extend(response['query']['backlinks'])\n",
    "\n",
    "while 'continue' in response:\n",
    "    url = url + '&blcontinue='+ response['continue']['blcontinue']\n",
    "    response = requests.get(url=url).json()\n",
    "    whatLinks.extend(response['query']['backlinks'])\n",
    "\n",
    "QswhatLinks = [v['title'] for v in whatLinks]\n",
    "QswhatLinks = set(QswhatLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All truthy statements with COVID-19 (Q84263196) as value.\n",
    "#https://w.wiki/KvZ\n",
    "\n",
    "# All statements with item, property, value and rank with COVID-19 (Q84263196) as value for qualifier.\n",
    "#https://w.wiki/KvX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what links to 2019–20 COVID-19 pandemic (Q81068910) in Wikidata\n",
    "whatLinks2 = []\n",
    "\n",
    "url = 'https://www.wikidata.org/w/api.php?action=query&format=json&list=backlinks&bltitle=Q81068910&bllimit=500&blnamespace=0'\n",
    "response = requests.get(url=url).json()\n",
    "whatLinks2.extend(response['query']['backlinks'])\n",
    "\n",
    "while 'continue' in response:\n",
    "    url = url + '&blcontinue='+ response['continue']['blcontinue']\n",
    "    response = requests.get(url=url).json()\n",
    "    whatLinks2.extend(response['query']['backlinks'])\n",
    "\n",
    "QswhatLinks2 = [v['title'] for v in whatLinks2]\n",
    "QswhatLinks2 = set(QswhatLinks2)\n",
    "\n",
    "QsLinksCovid = QswhatLinks.union(QswhatLinks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All truthy statements with 2019–20 COVID-19 pandemic (Q81068910) as value.\n",
    "#https://w.wiki/Kvd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>project</th>\n",
       "      <th>url</th>\n",
       "      <th>wikilink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>أثر جائحة فيروس كورونا على الدين 2019-20</td>\n",
       "      <td>ar.wikipedia</td>\n",
       "      <td>https://ar.wikipedia.org/wiki/%D8%A3%D8%AB%D8%...</td>\n",
       "      <td>[[ar:أثر جائحة فيروس كورونا على الدين 2019-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Impact of the COVID-19 pandemic on religion</td>\n",
       "      <td>en.wikipedia</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Impact_of_the_CO...</td>\n",
       "      <td>[[en:Impact of the COVID-19 pandemic on religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Impacto en la religión de la pandemia de enfer...</td>\n",
       "      <td>es.wikipedia</td>\n",
       "      <td>https://es.wikipedia.org/wiki/Impacto_en_la_re...</td>\n",
       "      <td>[[es:Impacto en la religión de la pandemia de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dampak pandemi koronavirus terhadap kegiatan k...</td>\n",
       "      <td>id.wikipedia</td>\n",
       "      <td>https://id.wikipedia.org/wiki/Dampak_pandemi_k...</td>\n",
       "      <td>[[id:Dampak pandemi koronavirus terhadap kegia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>코로나바이러스감염증-19 범유행이 종교에 준 영향</td>\n",
       "      <td>ko.wikipedia</td>\n",
       "      <td>https://ko.wikipedia.org/wiki/%EC%BD%94%EB%A1%...</td>\n",
       "      <td>[[ko:코로나바이러스감염증-19 범유행이 종교에 준 영향</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                page       project  \\\n",
       "0           أثر جائحة فيروس كورونا على الدين 2019-20  ar.wikipedia   \n",
       "1        Impact of the COVID-19 pandemic on religion  en.wikipedia   \n",
       "2  Impacto en la religión de la pandemia de enfer...  es.wikipedia   \n",
       "3  Dampak pandemi koronavirus terhadap kegiatan k...  id.wikipedia   \n",
       "4                        코로나바이러스감염증-19 범유행이 종교에 준 영향  ko.wikipedia   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://ar.wikipedia.org/wiki/%D8%A3%D8%AB%D8%...   \n",
       "1  https://en.wikipedia.org/wiki/Impact_of_the_CO...   \n",
       "2  https://es.wikipedia.org/wiki/Impacto_en_la_re...   \n",
       "3  https://id.wikipedia.org/wiki/Dampak_pandemi_k...   \n",
       "4  https://ko.wikipedia.org/wiki/%EC%BD%94%EB%A1%...   \n",
       "\n",
       "                                            wikilink  \n",
       "0      [[ar:أثر جائحة فيروس كورونا على الدين 2019-20  \n",
       "1   [[en:Impact of the COVID-19 pandemic on religion  \n",
       "2  [[es:Impacto en la religión de la pandemia de ...  \n",
       "3  [[id:Dampak pandemi koronavirus terhadap kegia...  \n",
       "4                   [[ko:코로나바이러스감염증-19 범유행이 종교에 준 영향  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Covid-related articles with their project\n",
    "COVID_RELATED_ARTICLES_PATH = \"data/COVID_related_pages_project.csv\"\n",
    "df_covid_articles = pd.read_csv(COVID_RELATED_ARTICLES_PATH)\n",
    "df_covid_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Original dataset limits\n",
    "\n",
    "When exploring our original dataset and the list above, we saw that some important articles were not considered. Indeed, articles no longer used at the time of the original analysis were not considered because these articles only redirected on other covid related pages and have no content anymore. However at the beginning of the pandemic, these articles linked to Covid-19 main pages and were viewed a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Topviews Analysis tool of wikidata(https://pageviews.wmcloud.org/topviews/?project=fr.wikipedia.org&platform=all-access&date=2020-03-31&excludes=) we found for example that for the french language the following 12 pages are obviously related to covid but not all of them are present in the list of covid articles because most of them have been depreciated and now redirect to other pages. For example, Pandémie de maladie à coronavirus de 2020 en France redirect to Pandémie de COVID-19 en France. Thus, not considering this pages can lead to a biased analysis of the interest for Covid-19 as it can be well under-estimated.\n",
    "\n",
    "\n",
    "![title](picture_fr_pageviews-31-03-2020.png)\n",
    "![title](redirect_example.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dataset outputs 38866 covid-related page views for the 31-03-2021 in french languages, which is much less that the views of the articles showed above totaling 315654 views.\n"
     ]
    }
   ],
   "source": [
    "#loading data\n",
    "agg = load_aggregated(\"data/aggregated_timeseries.json.gz\")\n",
    "df_agg = pd.DataFrame.from_dict(agg)\n",
    "\n",
    "#getting page views from fr.wikipedia covid-19 related articles\n",
    "df_fr_covid = (pd.DataFrame.from_dict(df_agg['fr']['covid']) + pd.DataFrame.from_dict(df_agg[\"fr.m\"]['covid']))['sum']\n",
    "print(\"The original dataset outputs {} covid-related page views for the 31-03-2021 in french languages, which is much less that the views of the articles showed above totaling 315654 views.\".format(int(df_fr_covid.loc[df_fr_covid.index == '2020-03-31'].values)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the aim of our project: analyse the \"digital\" propagation of Covid-19. We decided to extend the list of covid related articles by also considering the depreciated articles that now redirect on Covid-19 related articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Data extraction with extanded covid-19 related articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countries with specific language not spoken to much abroad\n",
    "DICT_COUNTRIES_OWN_LANG = {\"Italy\" : \"it\", \"Russia\": \"ru\", \"China\": \"zh\", \"India\": \"hi\", \"Albania\": \"sq\", \n",
    "\"Bangladesh\": \"bn\", \"Bostwana\": \"tn\", \"Cambogia\": \"km\", \"Croatia\": \"hr\", \"Greece\": \"el\", \"Sweden\": \"sv\", \"Finland\": \"fi\", \"Norway\": \"no\",\n",
    " \"Malaysian\": \"ms\", \"Israel\": \"he\", \"Lithuania\": \"lt\", \"Serbia\": \"sr\", \"Slovakia\": \"sk\", \"Slovania\": \"sl\", \"Turkey\": \"tr\", \"Ukrain\": \"uk\",\n",
    " \"Vietnam\": \"vi\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "'''\n",
    "Function to be used within data fetching script\n",
    "Inputs : \n",
    "    - json_obj : dict that was fetched in the given iteration of the script\n",
    "    - return_df : final df that will be return at the end of the script\n",
    "Output : concatenated df\n",
    "'''\n",
    "def json_to_df(json_obj, return_df):\n",
    "    #Create df from json with nested list\n",
    "    df = pd.DataFrame()\n",
    "    if json_obj.get('title') == \"Not found.\":\n",
    "        #print(\"Article not anymore in WikiData logs.\")\n",
    "        return pd.concat([return_df, df])\n",
    "    try:\n",
    "        df = pd.json_normalize(json_obj, record_path = ['items']).set_index(['timestamp']).drop(labels = ['project', 'granularity'\n",
    "        , 'access', 'agent', 'article'], axis = 1)\n",
    "    except:\n",
    "        print(\"Error json to df\")\n",
    "        print(json_obj)\n",
    "    #concatenation\n",
    "    return  pd.concat([return_df, df])\n",
    "\n",
    "'''\n",
    "Function to get the depreciated articles\n",
    "Inputs : \n",
    "    - title: title of the article where pages get redirected\n",
    "    - lang: language data to extract\n",
    "    - df_articles: list of covid-related article\n",
    "Output : list of articles name\n",
    "'''\n",
    "def get_redirect_articles(title, lang, df_articles):\n",
    "    S = requests.Session()\n",
    "\n",
    "    url = \"https://{}.wikipedia.org/w/api.php\".format(lang)\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"redirects\"\n",
    "    }\n",
    "    try:\n",
    "        #get request to get the article that redirect of title\n",
    "        r = S.get(url=url, params=PARAMS)\n",
    "        data = r.json()\n",
    "        if data['query'].get('pages') is None:\n",
    "            return []\n",
    "        pages = data[\"query\"][\"pages\"]\n",
    "        res = []\n",
    "        #iterate through dict to get the articles name\n",
    "        for k, v in pages.items():\n",
    "            if v.get(\"redirects\") is not None:\n",
    "                for re in v[\"redirects\"]:\n",
    "                    if re[\"title\"] not in df_articles.values:\n",
    "                        res.append(re[\"title\"])\n",
    "    except:\n",
    "        print(\"Error request Redirection\")\n",
    "        res = []\n",
    "    \n",
    "    return res\n",
    "\n",
    "'''\n",
    "Function to fetch the data using wikimedia api\n",
    "Inputs : \n",
    "    - languageCode : language data to extract\n",
    "    - begin_date\n",
    "    - end_date\n",
    "Output : concatenated df\n",
    "'''\n",
    "def wiki_to_df_extract(languageCode, begin_date, end_date):\n",
    "    #get covid article list\n",
    "    df_covid_articles = pd.read_csv(COVID_RELATED_ARTICLES_PATH)\n",
    "    #filter language we want to extract\n",
    "    df_covid_articles_country = df_covid_articles.loc[df_covid_articles.project == \"{}.wikipedia\".format(languageCode)]['page']\n",
    "    df_agg_country = pd.DataFrame()\n",
    "    redirect_art_list = []\n",
    "    #iterate through articles\n",
    "    for page in df_covid_articles_country:\n",
    "        #get list of depreciated articles\n",
    "        redirect_art_list = get_redirect_articles(page, languageCode, df_covid_articles_country)\n",
    "        redirect_art_list.append(page)\n",
    "        for page in redirect_art_list:\n",
    "            #getting name in url-friendly synthax\n",
    "            page = page.replace(' ', '_')\n",
    "            page = urllib.parse.quote(page)\n",
    "            page = page.replace('/', \"\"\"%2F\"\"\")\n",
    "            page = page.replace('?', \"\"\"%3F\"\"\")\n",
    "            url = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{}.wikipedia.org/all-access/user/{}/daily/{}/{}'.format(languageCode,page, begin_date, end_date)\n",
    "            #without head we get blocked from the api\n",
    "            header = {'User-Agent' : 'Robin Debalme (academic project; robin.debalme@epfl.ch; https://github.com/epfl-ada/ada-2022-project-thedatadiggers22)'}\n",
    "            try:\n",
    "                #get request to get a json of the page views of the given period\n",
    "                r = requests.get(url, headers = header).json()\n",
    "            except:\n",
    "                print(\"Error request Extract\")\n",
    "            #concat\n",
    "            df_agg_country = json_to_df(r, df_agg_country)\n",
    "    return df_agg_country.groupby(['timestamp'])['views'].sum().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60 languages with at least 10 articles related to covid.\n"
     ]
    }
   ],
   "source": [
    "df_mainLang = df_covid_articles.groupby(['project']).aggregate('count')\n",
    "df_mainLang = df_mainLang.loc[df_mainLang.page > 10]\n",
    "print(\"There are {} languages with at least 10 articles related to covid.\".format(str(df_mainLang.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_agg_c_own_lang = pd.DataFrame()\n",
    "df_tmp  = pd.DataFrame()\n",
    "for value in DICT_COUNTRIES_OWN_LANG:\n",
    "    #get covid related from 01-01-2020 to 01-01-2021 for each language\n",
    "    df_tmp = wiki_to_df_extract(DICT_COUNTRIES_OWN_LANG[value], '20200101', '20210101').rename({'views': value}, axis='columns')\n",
    "    df_covid_agg_c_own_lang = pd.concat([df_covid_agg_c_own_lang, df_tmp], axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_agg_c_own_lang.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Error request Extract\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n",
      "Article not anymore in WikiData logs.\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "#load original aggregated page views\n",
    "agg = load_aggregated(\"data/aggregated_timeseries.json.gz\")\n",
    "df_agg = pd.DataFrame.from_dict(agg)\n",
    "#load intervention in different countries\n",
    "interventions = pd.read_csv('data/interventions.csv')\n",
    "#removing english and catalan\n",
    "interventions = interventions.drop(interventions[interventions['lang']=='en'].index).drop(interventions[interventions['lang']=='ca'].index)\n",
    "df_covid_agg = pd.DataFrame()\n",
    "df_newM  = pd.DataFrame()\n",
    "lang = []\n",
    "#we considered only 4 language to compare with the new implementation\n",
    "countries = [\"de\", \"it\", \"fr\", \"ja\"]\n",
    "for value in countries:\n",
    "    #extract page views using new method\n",
    "    df_newM = wiki_to_df_extract(value, '20200101', '20200731').rename({'views': value}, axis='columns')\n",
    "    df_newM['date'] = df_newM.index\n",
    "    df_newM['date'] = df_newM['date'].apply(lambda s: pd.to_datetime(s[:8], format='%Y%m%d'))\n",
    "    df_newM = df_newM.set_index('date')\n",
    "\n",
    "    #Plot the evolution of the number of pages search per language\n",
    "    plt.plot(df_newM[value], label=str(value))\n",
    "    #extract page views using old method\n",
    "    df_oldM = pd.DataFrame.from_dict(df_agg[value]['covid']) + pd.DataFrame.from_dict(df_agg[value + \".m\"]['covid'])\n",
    "    df_oldM = df_oldM.loc[df_oldM.index > '2020-01-01'].drop(['percent', 'len'], axis= 1)\n",
    "    #Plot the evolution of the number of pages search per language\n",
    "    plt.plot(df_oldM, label=str(value) + \" original\")\n",
    "    #df_covid_agg = pd.concat([df_covid_agg, df_tmp], axis= 1)\n",
    "plt.legend()\n",
    "plt.title(\"#Covid page views per languages\")\n",
    "skip = 30 #To show only the date on x-axis every month\n",
    "plt.xticks(df_newM.index[::skip])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
